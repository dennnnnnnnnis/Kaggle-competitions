{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 21:11:43.655074: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-31 21:11:43.671580: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1735650703.691277  387003 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1735650703.696247  387003 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-31 21:11:43.714414: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, models, callbacks, losses, optimizers, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "GPU Devices:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPU Devices: \", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    seed = 2025\n",
    "    target_col = \"responder_6\"\n",
    "    # data_id is not included as it's not relavant\n",
    "    feature_cols = [f\"feature_{idx:02d}\" for idx in range(79)] \\\n",
    "        + [f\"responder_{idx}_lag_1\" for idx in range(9)]\n",
    "    categorical_cols = []\n",
    "    batch_size = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31646824, 103), (1643664, 103))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pl.scan_parquet(\"/root/autodl-tmp/jane-street-2024/training.parquet\").collect().to_pandas()\n",
    "valid = pl.scan_parquet(\"/root/autodl-tmp/jane-street-2024/validation.parquet\").collect().to_pandas()\n",
    "train.shape, valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33290488, 103)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trick of boosting LB score, data leakage on the validation set\n",
    "train = pd.concat([train, valid]).reset_index(drop=True)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>symbol_id</th>\n",
       "      <th>weight</th>\n",
       "      <th>feature_00</th>\n",
       "      <th>feature_01</th>\n",
       "      <th>feature_02</th>\n",
       "      <th>feature_03</th>\n",
       "      <th>feature_04</th>\n",
       "      <th>...</th>\n",
       "      <th>partition_id</th>\n",
       "      <th>responder_0_lag_1</th>\n",
       "      <th>responder_1_lag_1</th>\n",
       "      <th>responder_2_lag_1</th>\n",
       "      <th>responder_3_lag_1</th>\n",
       "      <th>responder_4_lag_1</th>\n",
       "      <th>responder_5_lag_1</th>\n",
       "      <th>responder_6_lag_1</th>\n",
       "      <th>responder_7_lag_1</th>\n",
       "      <th>responder_8_lag_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21489858</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.324375</td>\n",
       "      <td>-0.276179</td>\n",
       "      <td>-0.655325</td>\n",
       "      <td>-0.404810</td>\n",
       "      <td>-0.349785</td>\n",
       "      <td>-2.882722</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.504762</td>\n",
       "      <td>-0.758082</td>\n",
       "      <td>-0.795381</td>\n",
       "      <td>0.124352</td>\n",
       "      <td>0.036644</td>\n",
       "      <td>0.296034</td>\n",
       "      <td>0.321345</td>\n",
       "      <td>0.207008</td>\n",
       "      <td>0.598205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21489859</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.711303</td>\n",
       "      <td>-0.418316</td>\n",
       "      <td>-0.762019</td>\n",
       "      <td>-0.433680</td>\n",
       "      <td>-0.616798</td>\n",
       "      <td>-2.577970</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.390356</td>\n",
       "      <td>0.187457</td>\n",
       "      <td>-0.609749</td>\n",
       "      <td>-0.128713</td>\n",
       "      <td>-0.070782</td>\n",
       "      <td>-0.449838</td>\n",
       "      <td>-0.532821</td>\n",
       "      <td>-0.170380</td>\n",
       "      <td>-0.582633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21489860</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.028847</td>\n",
       "      <td>-0.724897</td>\n",
       "      <td>-1.223187</td>\n",
       "      <td>-0.452174</td>\n",
       "      <td>-0.523907</td>\n",
       "      <td>-2.617430</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.420631</td>\n",
       "      <td>0.208989</td>\n",
       "      <td>-0.563919</td>\n",
       "      <td>-0.031235</td>\n",
       "      <td>-0.015218</td>\n",
       "      <td>0.298194</td>\n",
       "      <td>0.166585</td>\n",
       "      <td>0.105961</td>\n",
       "      <td>0.160067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21489861</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.099438</td>\n",
       "      <td>-0.717159</td>\n",
       "      <td>-0.259479</td>\n",
       "      <td>-0.522695</td>\n",
       "      <td>-0.066547</td>\n",
       "      <td>-2.712632</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.456872</td>\n",
       "      <td>1.149381</td>\n",
       "      <td>0.179879</td>\n",
       "      <td>0.108469</td>\n",
       "      <td>0.089928</td>\n",
       "      <td>-0.113625</td>\n",
       "      <td>-0.033634</td>\n",
       "      <td>-0.246281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21489862</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.166049</td>\n",
       "      <td>-0.377845</td>\n",
       "      <td>-0.360645</td>\n",
       "      <td>-0.641121</td>\n",
       "      <td>-0.508439</td>\n",
       "      <td>-2.661481</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.074267</td>\n",
       "      <td>0.026688</td>\n",
       "      <td>0.697218</td>\n",
       "      <td>-0.044805</td>\n",
       "      <td>-0.018261</td>\n",
       "      <td>0.148812</td>\n",
       "      <td>-0.173818</td>\n",
       "      <td>-0.044245</td>\n",
       "      <td>-0.285043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  date_id  time_id  symbol_id    weight  feature_00  feature_01  \\\n",
       "0  21489858     1000        0          0  3.324375   -0.276179   -0.655325   \n",
       "1  21489859     1000        0          1  4.711303   -0.418316   -0.762019   \n",
       "2  21489860     1000        0          2  3.028847   -0.724897   -1.223187   \n",
       "3  21489861     1000        0          3  2.099438   -0.717159   -0.259479   \n",
       "4  21489862     1000        0          4  3.166049   -0.377845   -0.360645   \n",
       "\n",
       "   feature_02  feature_03  feature_04  ...  partition_id  responder_0_lag_1  \\\n",
       "0   -0.404810   -0.349785   -2.882722  ...             5          -0.504762   \n",
       "1   -0.433680   -0.616798   -2.577970  ...             5          -0.390356   \n",
       "2   -0.452174   -0.523907   -2.617430  ...             5          -0.420631   \n",
       "3   -0.522695   -0.066547   -2.712632  ...             5           5.000000   \n",
       "4   -0.641121   -0.508439   -2.661481  ...             5          -0.074267   \n",
       "\n",
       "   responder_1_lag_1  responder_2_lag_1  responder_3_lag_1  responder_4_lag_1  \\\n",
       "0          -0.758082          -0.795381           0.124352           0.036644   \n",
       "1           0.187457          -0.609749          -0.128713          -0.070782   \n",
       "2           0.208989          -0.563919          -0.031235          -0.015218   \n",
       "3           0.456872           1.149381           0.179879           0.108469   \n",
       "4           0.026688           0.697218          -0.044805          -0.018261   \n",
       "\n",
       "   responder_5_lag_1  responder_6_lag_1  responder_7_lag_1  responder_8_lag_1  \n",
       "0           0.296034           0.321345           0.207008           0.598205  \n",
       "1          -0.449838          -0.532821          -0.170380          -0.582633  \n",
       "2           0.298194           0.166585           0.105961           0.160067  \n",
       "3           0.089928          -0.113625          -0.033634          -0.246281  \n",
       "4           0.148812          -0.173818          -0.044245          -0.285043  \n",
       "\n",
       "[5 rows x 103 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33290488, 88),\n",
       " (33290488,),\n",
       " (33290488,),\n",
       " (1643664, 88),\n",
       " (1643664,),\n",
       " (1643664,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train[ CONFIG.feature_cols ]\n",
    "X_train = X_train.ffill().fillna(0).values\n",
    "y_train = train[ CONFIG.target_col ].values\n",
    "w_train = train[\"weight\"].values\n",
    "\n",
    "X_valid = valid[ CONFIG.feature_cols ]\n",
    "X_valid = X_valid.ffill().fillna(0).values\n",
    "y_valid = valid[ CONFIG.target_col ].values\n",
    "w_valid = valid[\"weight\"].values\n",
    "\n",
    "X_train.shape, y_train.shape, w_train.shape, X_valid.shape, y_valid.shape, w_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = X_train.mean()\n",
    "stds = X_train.std()\n",
    "\n",
    "X_train = (X_train - means) / stds\n",
    "X_valid = (X_valid - means) / stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('scaler_params.npz', means=means, stds=stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, X, y, w, batch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.w = w\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = idx * self.batch_size\n",
    "        end_idx = min((idx + 1) * self.batch_size, len(self.X))\n",
    "        batch_X = self.X[start_idx:end_idx]\n",
    "        batch_y = self.y[start_idx:end_idx]\n",
    "        batch_w = self.w[start_idx:end_idx]\n",
    "        \n",
    "        return (\n",
    "            batch_X,\n",
    "            {\n",
    "                'decoder_output': batch_X,\n",
    "                'prediction': tf.expand_dims(batch_y, -1)\n",
    "            },\n",
    "            batch_w\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_autoencoder(input_shape):\n",
    "    \n",
    "    # Stage 1: Encoder Input\n",
    "    encoder_input = layers.Input(shape=input_shape)\n",
    "    # prevent overfitting\n",
    "    x0 = layers.BatchNormalization()(encoder_input)\n",
    "\n",
    "    # for data augmentation and to prevent overfitting\n",
    "    encoded = layers.GaussianNoise(0.05)(x0)\n",
    "    # Stage 2: Dense => ReLU\n",
    "    encoded = layers.Dense(512)(encoded)\n",
    "    encoded = layers.Activation(\"swish\")(encoded)\n",
    "    encoded = layers.BatchNormalization()(encoded)\n",
    "    encoded = layers.Dropout(0.3)(encoded)\n",
    "    \n",
    "    # Stage 3: Dense => ReLU\n",
    "    encoded = layers.Dense(256)(encoded)\n",
    "    encoded = layers.Activation(\"swish\")(encoded)\n",
    "    encoded = layers.BatchNormalization()(encoded)\n",
    "    encoded = layers.Dropout(0.3)(encoded)\n",
    "    \n",
    "    # Stage 4: Dense => ReLU\n",
    "    encoded = layers.Dense(64)(encoded)\n",
    "    encoded = layers.Activation(\"swish\")(encoded)\n",
    "    encoded = layers.BatchNormalization()(encoded)\n",
    "    encoded = layers.Dropout(0.3)(encoded)\n",
    "\n",
    "    # Stage 4: Dense => ReLU\n",
    "    encoded = layers.Dense(32)(encoded)\n",
    "    encoded = layers.Activation(\"swish\")(encoded)\n",
    "    encoded = layers.BatchNormalization()(encoded)\n",
    "    encoded = layers.Dropout(0.3)(encoded)\n",
    "\n",
    "    # Stage 5: 32 features as the final output, same as pca\n",
    "    encoded = layers.Dense(16)(encoded)\n",
    "    encoded = layers.Activation(\"swish\", name=\"encoder_output\")(encoded)\n",
    "    \n",
    "    ##################### Encoder model ########################\n",
    "    encoder = models.Model(inputs=encoder_input, outputs=encoded)\n",
    "    ############################################################\n",
    "    \n",
    "    # # Stage 6: Decoder Input\n",
    "    # decoder_input = layers.Input(shape=(32,))\n",
    "\n",
    "    # Stage 7: Dense => ReLU\n",
    "    decoded = layers.Dense(32)(encoded)\n",
    "    decoded = layers.Activation(\"swish\")(decoded)\n",
    "    decoded = layers.BatchNormalization()(decoded)\n",
    "    decoded = layers.Dropout(0.3)(decoded)\n",
    "\n",
    "    # Stage 7: Dense => ReLU\n",
    "    decoded = layers.Dense(64)(encoded)\n",
    "    decoded = layers.Activation(\"swish\")(decoded)\n",
    "    decoded = layers.BatchNormalization()(decoded)\n",
    "    decoded = layers.Dropout(0.3)(decoded)\n",
    "    \n",
    "    # Stage 8: Dense => ReLU\n",
    "    decoded = layers.Dense(256)(decoded)\n",
    "    decoded = layers.Activation(\"swish\")(decoded)\n",
    "    decoded = layers.BatchNormalization()(decoded)\n",
    "    decoded = layers.Dropout(0.3)(decoded)\n",
    "    \n",
    "    # Stage 9: Dense => ReLU\n",
    "    decoded = layers.Dense(512)(decoded)\n",
    "    decoded = layers.Activation(\"swish\")(decoded)\n",
    "    decoded = layers.BatchNormalization()(decoded)\n",
    "    decoded = layers.Dropout(0.3)(decoded)\n",
    "    \n",
    "    # Stage 10: Dense => ReLU\n",
    "    decoded = layers.Dense(input_shape[0], name=\"decoder_output\")(decoded)\n",
    "    # decoded = layers.Activation(\"swish\", name=\"decoder_output\")(decoded)\n",
    "    \n",
    "    ###################### Decoder Model #######################\n",
    "    decoder = models.Model(inputs=encoded, outputs=decoded)\n",
    "    ############################################################\n",
    "    \n",
    "    ##################### autoencoder model workflow #####################\n",
    "    # autoencoder_input = layers.Input(input_shape)\n",
    "    # encoded_autoencoder = encoder(autoencoder_input) # encoded\n",
    "    # decoded_autoencoder = decoder(encoded_autoencoder) # decoded\n",
    "\n",
    "    ################# Prediction branch from bottleneck ####################\n",
    "    prediction = layers.Dense(64, activation=\"swish\")(encoded)\n",
    "    prediction = layers.BatchNormalization()(prediction)\n",
    "    prediction = layers.Dropout(0.3)(prediction)\n",
    "    prediction = layers.Dense(1)(prediction)\n",
    "    prediction = layers.Activation('tanh')(prediction)\n",
    "    prediction_output = layers.Lambda(lambda x: 5 * x, name='prediction')(prediction)\n",
    "\n",
    "    autoencoder = models.Model(inputs=encoder_input, outputs=[decoded, prediction_output])\n",
    "    \n",
    "    autoencoder.compile(optimizer='adam',\n",
    "                loss={'decoder_output': 'mse',\n",
    "                      'prediction': 'mse'})\n",
    "    \n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create generators for training and validation\n",
    "train_generator = DataGenerator(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    w_train,\n",
    "    CONFIG.batch_size,\n",
    "    # workers=8,\n",
    "    # use_multiprocessing=True\n",
    ")\n",
    "valid_generator = DataGenerator(\n",
    "    X_valid,\n",
    "    y_valid,\n",
    "    w_valid,\n",
    "    CONFIG.batch_size,\n",
    "    # workers=8,\n",
    "    # use_multiprocessing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1735651020.445922  387003 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22456 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:4f:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "autoencoder, encoder = dense_autoencoder((X_train.shape[1], ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1735651032.989537  387590 service.cc:148] XLA service 0x7f664c002b20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1735651032.989627  387590 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-12-31 21:17:13.225840: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1735651033.857985  387590 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2024-12-31 21:17:17.093823: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4376', 156 bytes spill stores, 156 bytes spill loads\n",
      "\n",
      "2024-12-31 21:17:17.112847: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4374', 108 bytes spill stores, 108 bytes spill loads\n",
      "\n",
      "2024-12-31 21:17:17.125575: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4376_0', 2000 bytes spill stores, 1940 bytes spill loads\n",
      "\n",
      "2024-12-31 21:17:17.137886: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4376', 2992 bytes spill stores, 2976 bytes spill loads\n",
      "\n",
      "2024-12-31 21:17:17.143206: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4374', 88 bytes spill stores, 88 bytes spill loads\n",
      "\n",
      "2024-12-31 21:17:17.184766: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4376', 4196 bytes spill stores, 4124 bytes spill loads\n",
      "\n",
      "2024-12-31 21:17:17.406595: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4376_0', 10420 bytes spill stores, 10780 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  18/8128\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 10ms/step - decoder_output_loss: 5.5749 - loss: 26.3826 - prediction_loss: 20.8077"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1735651043.275547  387590 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7200/8128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - decoder_output_loss: 0.3481 - loss: 2.5713 - prediction_loss: 2.2231"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 21:18:24.789782: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4374', 172 bytes spill stores, 172 bytes spill loads\n",
      "\n",
      "2024-12-31 21:18:24.791096: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4374', 104 bytes spill stores, 104 bytes spill loads\n",
      "\n",
      "2024-12-31 21:18:24.848933: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4376', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2024-12-31 21:18:24.865285: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5_0', 252 bytes spill stores, 252 bytes spill loads\n",
      "\n",
      "2024-12-31 21:18:24.884349: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4376', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2024-12-31 21:18:24.908728: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4376', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2024-12-31 21:18:24.922145: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4376_0', 224 bytes spill stores, 228 bytes spill loads\n",
      "\n",
      "2024-12-31 21:18:25.019157: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4376_0', 696 bytes spill stores, 648 bytes spill loads\n",
      "\n",
      "2024-12-31 21:18:25.035438: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4376', 1484 bytes spill stores, 1344 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8128/8128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 10ms/step - decoder_output_loss: 0.3207 - loss: 2.4788 - prediction_loss: 2.1580 - val_decoder_output_loss: 0.0142 - val_loss: 1.2021 - val_prediction_loss: 1.1884 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m8128/8128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 8ms/step - decoder_output_loss: 0.0219 - loss: 1.5418 - prediction_loss: 1.5199 - val_decoder_output_loss: 0.0084 - val_loss: 1.1815 - val_prediction_loss: 1.1737 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m8128/8128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 8ms/step - decoder_output_loss: 0.0137 - loss: 1.4833 - prediction_loss: 1.4696 - val_decoder_output_loss: 0.0045 - val_loss: 1.1762 - val_prediction_loss: 1.1724 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m8128/8128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 8ms/step - decoder_output_loss: 0.0121 - loss: 1.5109 - prediction_loss: 1.4989 - val_decoder_output_loss: 0.0065 - val_loss: 1.1882 - val_prediction_loss: 1.1824 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m8128/8128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 8ms/step - decoder_output_loss: 0.0121 - loss: 1.5072 - prediction_loss: 1.4951 - val_decoder_output_loss: 0.0064 - val_loss: 1.2135 - val_prediction_loss: 1.2080 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m8128/8128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 8ms/step - decoder_output_loss: 0.0121 - loss: 1.4489 - prediction_loss: 1.4368 - val_decoder_output_loss: 0.0066 - val_loss: 1.2363 - val_prediction_loss: 1.2307 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m8128/8128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 8ms/step - decoder_output_loss: 0.0127 - loss: 1.4471 - prediction_loss: 1.4344 - val_decoder_output_loss: 0.0074 - val_loss: 1.2252 - val_prediction_loss: 1.2186 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m8122/8128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - decoder_output_loss: 0.0128 - loss: 1.4243 - prediction_loss: 1.4114\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m8128/8128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 8ms/step - decoder_output_loss: 0.0128 - loss: 1.4243 - prediction_loss: 1.4114 - val_decoder_output_loss: 0.0064 - val_loss: 1.2628 - val_prediction_loss: 1.2572 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m8128/8128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 8ms/step - decoder_output_loss: 0.0124 - loss: 1.4013 - prediction_loss: 1.3889 - val_decoder_output_loss: 0.0049 - val_loss: 1.2258 - val_prediction_loss: 1.2217 - learning_rate: 5.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m8128/8128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 7ms/step - decoder_output_loss: 0.0122 - loss: 1.3907 - prediction_loss: 1.3786 - val_decoder_output_loss: 0.0080 - val_loss: 1.2402 - val_prediction_loss: 1.2331 - learning_rate: 5.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m8128/8128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 8ms/step - decoder_output_loss: 0.0123 - loss: 1.3845 - prediction_loss: 1.3722 - val_decoder_output_loss: 0.0048 - val_loss: 1.2329 - val_prediction_loss: 1.2288 - learning_rate: 5.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m8128/8128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 7ms/step - decoder_output_loss: 0.0121 - loss: 1.3949 - prediction_loss: 1.3828 - val_decoder_output_loss: 0.0050 - val_loss: 1.2377 - val_prediction_loss: 1.2334 - learning_rate: 5.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m8122/8128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - decoder_output_loss: 0.0122 - loss: 1.3807 - prediction_loss: 1.3684\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m8128/8128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 7ms/step - decoder_output_loss: 0.0122 - loss: 1.3807 - prediction_loss: 1.3684 - val_decoder_output_loss: 0.0057 - val_loss: 1.2304 - val_prediction_loss: 1.2255 - learning_rate: 5.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m8128/8128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 7ms/step - decoder_output_loss: 0.0120 - loss: 1.3645 - prediction_loss: 1.3525 - val_decoder_output_loss: 0.0051 - val_loss: 1.2294 - val_prediction_loss: 1.2251 - learning_rate: 2.5000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m8128/8128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 7ms/step - decoder_output_loss: 0.0118 - loss: 1.3352 - prediction_loss: 1.3234 - val_decoder_output_loss: 0.0049 - val_loss: 1.2360 - val_prediction_loss: 1.2319 - learning_rate: 2.5000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m8128/8128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 8ms/step - decoder_output_loss: 0.0119 - loss: 1.3554 - prediction_loss: 1.3435 - val_decoder_output_loss: 0.0052 - val_loss: 1.2420 - val_prediction_loss: 1.2376 - learning_rate: 2.5000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m8128/8128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 7ms/step - decoder_output_loss: 0.0118 - loss: 1.3617 - prediction_loss: 1.3499 - val_decoder_output_loss: 0.0054 - val_loss: 1.2451 - val_prediction_loss: 1.2406 - learning_rate: 2.5000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m8128/8128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - decoder_output_loss: 0.0120 - loss: 1.3504 - prediction_loss: 1.3384\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m8128/8128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 7ms/step - decoder_output_loss: 0.0120 - loss: 1.3504 - prediction_loss: 1.3384 - val_decoder_output_loss: 0.0064 - val_loss: 1.2459 - val_prediction_loss: 1.2403 - learning_rate: 2.5000e-04\n",
      "Epoch 18: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n"
     ]
    }
   ],
   "source": [
    "ckp = callbacks.ModelCheckpoint(\n",
    "    \"best-js-autoen.weights.h5\", \n",
    "    monitor='val_decoder_output_loss', \n",
    "    verbose=0, \n",
    "    save_best_only=True, \n",
    "    save_weights_only=True, \n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Add learning rate scheduling\n",
    "lr_scheduler = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_decoder_output_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Update early stopping\n",
    "es = callbacks.EarlyStopping(\n",
    "    monitor='val_decoder_output_loss',\n",
    "    min_delta=1e-4,\n",
    "    patience=15,  # Increased patience\n",
    "    mode='min',\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train with updated callbacks\n",
    "history = autoencoder.fit(\n",
    "    train_generator,\n",
    "    validation_data=valid_generator,\n",
    "    epochs=100,\n",
    "    callbacks=[ckp, es, lr_scheduler],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 20:21:06.999121: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-31 20:21:07.100098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22456 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:4f:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "autoencoder, encoder = dense_autoencoder((X_train.shape[1],))\n",
    "autoencoder.load_weights(\"/root/autodl-tmp/jane-street-2024/best-js-autoen.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8128/8128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "encoded_train = encoder.predict(X_train, batch_size=4096)\n",
    "encoded_valid = encoder.predict(X_valid, batch_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame with meaningful column names\n",
    "encoded_train_df = pd.DataFrame(\n",
    "    encoded_train,\n",
    "    columns=[f'encoded_feature_{i}' for i in range(encoded_train.shape[1])]\n",
    ")\n",
    "encoded_valid_df = pd.DataFrame(\n",
    "    encoded_valid,\n",
    "    columns=[f'encoded_feature_{i}' for i in range(encoded_valid.shape[1])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add target column and weights\n",
    "encoded_train_df[CONFIG.target_col] = train[CONFIG.target_col]\n",
    "encoded_train_df['weight'] = train['weight']\n",
    "encoded_train_df['symbol_id'] = train['symbol_id']\n",
    "encoded_train_df['date_id'] = train['date_id']\n",
    "encoded_train_df['time_id'] = train['time_id']\n",
    "\n",
    "encoded_valid_df[CONFIG.target_col] = valid[CONFIG.target_col]\n",
    "encoded_valid_df['weight'] = valid['weight']\n",
    "encoded_valid_df['symbol_id'] = valid['symbol_id']\n",
    "encoded_valid_df['date_id'] = valid['date_id']\n",
    "encoded_valid_df['time_id'] = valid['time_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoded_feature_0</th>\n",
       "      <th>encoded_feature_1</th>\n",
       "      <th>encoded_feature_2</th>\n",
       "      <th>encoded_feature_3</th>\n",
       "      <th>encoded_feature_4</th>\n",
       "      <th>encoded_feature_5</th>\n",
       "      <th>encoded_feature_6</th>\n",
       "      <th>encoded_feature_7</th>\n",
       "      <th>encoded_feature_8</th>\n",
       "      <th>encoded_feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>encoded_feature_11</th>\n",
       "      <th>encoded_feature_12</th>\n",
       "      <th>encoded_feature_13</th>\n",
       "      <th>encoded_feature_14</th>\n",
       "      <th>encoded_feature_15</th>\n",
       "      <th>responder_6</th>\n",
       "      <th>weight</th>\n",
       "      <th>symbol_id</th>\n",
       "      <th>date_id</th>\n",
       "      <th>time_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.168524</td>\n",
       "      <td>-0.057372</td>\n",
       "      <td>0.102818</td>\n",
       "      <td>-0.243955</td>\n",
       "      <td>-0.202454</td>\n",
       "      <td>-0.104967</td>\n",
       "      <td>0.321823</td>\n",
       "      <td>-0.222371</td>\n",
       "      <td>-0.262240</td>\n",
       "      <td>-0.180430</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.258119</td>\n",
       "      <td>-0.242541</td>\n",
       "      <td>-0.157287</td>\n",
       "      <td>-0.145760</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.496563</td>\n",
       "      <td>3.324375</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.162565</td>\n",
       "      <td>-0.060451</td>\n",
       "      <td>0.094373</td>\n",
       "      <td>-0.243832</td>\n",
       "      <td>-0.207173</td>\n",
       "      <td>-0.107619</td>\n",
       "      <td>0.317791</td>\n",
       "      <td>-0.222770</td>\n",
       "      <td>-0.263774</td>\n",
       "      <td>-0.184074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.257707</td>\n",
       "      <td>-0.245912</td>\n",
       "      <td>-0.156141</td>\n",
       "      <td>-0.148324</td>\n",
       "      <td>-0.013658</td>\n",
       "      <td>0.529877</td>\n",
       "      <td>4.711303</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.164049</td>\n",
       "      <td>-0.259337</td>\n",
       "      <td>-0.201851</td>\n",
       "      <td>-0.278407</td>\n",
       "      <td>-0.252687</td>\n",
       "      <td>-0.257245</td>\n",
       "      <td>0.225601</td>\n",
       "      <td>-0.249530</td>\n",
       "      <td>-0.273658</td>\n",
       "      <td>-0.244420</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042617</td>\n",
       "      <td>-0.223356</td>\n",
       "      <td>-0.231683</td>\n",
       "      <td>-0.271294</td>\n",
       "      <td>0.407031</td>\n",
       "      <td>0.746983</td>\n",
       "      <td>3.028847</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.271798</td>\n",
       "      <td>-0.055786</td>\n",
       "      <td>-0.239978</td>\n",
       "      <td>-0.229345</td>\n",
       "      <td>-0.060237</td>\n",
       "      <td>-0.220229</td>\n",
       "      <td>0.733118</td>\n",
       "      <td>-0.208833</td>\n",
       "      <td>-0.147414</td>\n",
       "      <td>-0.217443</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012005</td>\n",
       "      <td>-0.107234</td>\n",
       "      <td>-0.278464</td>\n",
       "      <td>-0.162567</td>\n",
       "      <td>-0.258254</td>\n",
       "      <td>0.941218</td>\n",
       "      <td>2.099438</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.269069</td>\n",
       "      <td>-0.106385</td>\n",
       "      <td>-0.235731</td>\n",
       "      <td>-0.240414</td>\n",
       "      <td>-0.079836</td>\n",
       "      <td>-0.237433</td>\n",
       "      <td>0.665825</td>\n",
       "      <td>-0.209488</td>\n",
       "      <td>-0.180263</td>\n",
       "      <td>-0.203104</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017777</td>\n",
       "      <td>-0.128438</td>\n",
       "      <td>-0.277465</td>\n",
       "      <td>-0.186881</td>\n",
       "      <td>-0.277370</td>\n",
       "      <td>0.204584</td>\n",
       "      <td>3.166049</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   encoded_feature_0  encoded_feature_1  encoded_feature_2  encoded_feature_3  \\\n",
       "0          -0.168524          -0.057372           0.102818          -0.243955   \n",
       "1          -0.162565          -0.060451           0.094373          -0.243832   \n",
       "2          -0.164049          -0.259337          -0.201851          -0.278407   \n",
       "3          -0.271798          -0.055786          -0.239978          -0.229345   \n",
       "4          -0.269069          -0.106385          -0.235731          -0.240414   \n",
       "\n",
       "   encoded_feature_4  encoded_feature_5  encoded_feature_6  encoded_feature_7  \\\n",
       "0          -0.202454          -0.104967           0.321823          -0.222371   \n",
       "1          -0.207173          -0.107619           0.317791          -0.222770   \n",
       "2          -0.252687          -0.257245           0.225601          -0.249530   \n",
       "3          -0.060237          -0.220229           0.733118          -0.208833   \n",
       "4          -0.079836          -0.237433           0.665825          -0.209488   \n",
       "\n",
       "   encoded_feature_8  encoded_feature_9  ...  encoded_feature_11  \\\n",
       "0          -0.262240          -0.180430  ...           -0.258119   \n",
       "1          -0.263774          -0.184074  ...           -0.257707   \n",
       "2          -0.273658          -0.244420  ...           -0.042617   \n",
       "3          -0.147414          -0.217443  ...           -0.012005   \n",
       "4          -0.180263          -0.203104  ...           -0.017777   \n",
       "\n",
       "   encoded_feature_12  encoded_feature_13  encoded_feature_14  \\\n",
       "0           -0.242541           -0.157287           -0.145760   \n",
       "1           -0.245912           -0.156141           -0.148324   \n",
       "2           -0.223356           -0.231683           -0.271294   \n",
       "3           -0.107234           -0.278464           -0.162567   \n",
       "4           -0.128438           -0.277465           -0.186881   \n",
       "\n",
       "   encoded_feature_15  responder_6    weight  symbol_id  date_id  time_id  \n",
       "0            0.005600     0.496563  3.324375          0     1000        0  \n",
       "1           -0.013658     0.529877  4.711303          1     1000        0  \n",
       "2            0.407031     0.746983  3.028847          2     1000        0  \n",
       "3           -0.258254     0.941218  2.099438          3     1000        0  \n",
       "4           -0.277370     0.204584  3.166049          4     1000        0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to parquet for efficient storage\n",
    "encoded_train_df.to_parquet('encoded_train.parquet')\n",
    "encoded_valid_df.to_parquet('encoded_valid.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_ae_mlp(num_columns, hidden_units, dropout_rates, lr = 1e-3):\n",
    "#     # Input layer and initial normalization\n",
    "#     inp = layers.Input(shape = (num_columns, ))\n",
    "#     x0 = layers.BatchNormalization()(inp)\n",
    "    \n",
    "#     # Encoder\n",
    "#     encoder = layers.GaussianNoise(dropout_rates[0])(x0)\n",
    "#     encoder = layers.Dense(hidden_units[0])(encoder)\n",
    "#     encoder = layers.BatchNormalization()(encoder)\n",
    "#     encoder = layers.Activation('swish')(encoder)\n",
    "    \n",
    "#     # Decoder\n",
    "#     decoder = layers.Dropout(dropout_rates[1])(encoder)\n",
    "#     decoder = layers.Dense(num_columns, name = 'decoder')(decoder)\n",
    "\n",
    "#     # Takes decoder output and makes predictions\n",
    "#     x_ae = layers.Dense(hidden_units[1])(decoder)\n",
    "#     x_ae = layers.BatchNormalization()(x_ae)\n",
    "#     x_ae = layers.Activation('swish')(x_ae)\n",
    "#     x_ae = layers.Dropout(dropout_rates[2])(x_ae)\n",
    "\n",
    "#     # out_ae = layers.Dense(num_labels, activation = 'sigmoid', name = 'ae_action')(x_ae)\n",
    "#     x_ae = layers.Dense(1)(x_ae)\n",
    "#     x_ae = layers.Activation('tanh')(x_ae)\n",
    "#     out_ae = layers.Lambda(lambda x: 5 * x, name='ae_reg')(x_ae)\n",
    "    \n",
    "#     # # Combines original normalized input with encoded representation\n",
    "#     # x = layers.Concatenate()([x0, encoder])\n",
    "#     # x = layers.BatchNormalization()(x)\n",
    "#     # x = layers.Dropout(dropout_rates[3])(x)\n",
    "    \n",
    "#     # # Deep network for main classification\n",
    "#     # for i in range(2, len(hidden_units)):\n",
    "#     #     x = layers.Dense(hidden_units[i])(x)\n",
    "#     #     x = layers.BatchNormalization()(x)\n",
    "#     #     x = layers.Activation('swish')(x)\n",
    "#     #     x = layers.Dropout(dropout_rates[i + 2])(x)\n",
    "        \n",
    "#     # # out = layers.Dense(num_labels, activation = 'sigmoid', name = 'action')(x)\n",
    "#     # x = layers.Dense(1)(x)\n",
    "#     # x = layers.Activation('tanh')(x)\n",
    "#     # out = layers.Lambda(lambda x: 5 * x, name='reg')(x)\n",
    "    \n",
    "#     # model definition and compile\n",
    "#     # model = models.Model(inputs = inp, outputs = [decoder, out_ae, out])\n",
    "#     model = models.Model(inputs = inp, outputs = [decoder, out_ae])\n",
    "#     model.compile(optimizer = optimizers.Adam(learning_rate = lr),\n",
    "#                   loss = {'decoder': losses.MeanSquaredError(), \n",
    "#                           'ae_reg': losses.MeanSquaredError(),\n",
    "#                           # 'action': losses.BinaryCrossentropy(label_smoothing = ls), \n",
    "#                          },\n",
    "#                   metrics = {'decoder': metrics.MeanAbsoluteError(name = 'MAE'), \n",
    "#                              'ae_reg': metrics.MeanSquaredError(name='MSE'), \n",
    "#                              # 'action': metrics.AUC(name = 'AUC'), \n",
    "#                             }, \n",
    "#                  )\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_ae_mlp(num_columns, hidden_units, dropout_rates, lr=1e-3):\n",
    "#     # Input layer and initial normalization\n",
    "#     inp = layers.Input(shape=(num_columns,))\n",
    "#     x0 = layers.BatchNormalization()(inp)\n",
    "    \n",
    "#     # Encoder - make it deeper and add regularization\n",
    "#     encoder = layers.GaussianNoise(dropout_rates[0])(x0)\n",
    "#     encoder = layers.Dense(hidden_units[0], kernel_regularizer=keras.regularizers.l2(1e-5))(encoder)\n",
    "#     encoder = layers.BatchNormalization()(encoder)\n",
    "#     encoder = layers.Activation('swish')(encoder)\n",
    "#     encoder = layers.Dropout(dropout_rates[1])(encoder)\n",
    "    \n",
    "#     # Add another encoder layer\n",
    "#     encoder = layers.Dense(hidden_units[0]//2, kernel_regularizer=keras.regularizers.l2(1e-5))(encoder)\n",
    "#     encoder = layers.BatchNormalization()(encoder)\n",
    "#     encoder = layers.Activation('swish')(encoder)\n",
    "    \n",
    "#     # Decoder with skip connection\n",
    "#     decoder = layers.Dense(num_columns)(encoder)\n",
    "#     decoder = layers.Add(name='decoder')([decoder, x0])  # Skip connection to help reconstruction\n",
    "    \n",
    "#     # Regression path - make it more focused on the regression task\n",
    "#     x_ae = layers.Dense(hidden_units[1])(decoder)\n",
    "#     x_ae = layers.BatchNormalization()(x_ae)\n",
    "#     x_ae = layers.Activation('swish')(x_ae)\n",
    "#     x_ae = layers.Dropout(dropout_rates[2])(x_ae)\n",
    "    \n",
    "#     # Additional layer for regression\n",
    "#     x_ae = layers.Dense(hidden_units[1]//2)(x_ae)\n",
    "#     x_ae = layers.BatchNormalization()(x_ae)\n",
    "#     x_ae = layers.Activation('swish')(x_ae)\n",
    "    \n",
    "#     # Output with tanh and scaling\n",
    "#     x_ae = layers.Dense(1)(x_ae)\n",
    "#     x_ae = layers.Activation('tanh')(x_ae)\n",
    "#     out_ae = layers.Lambda(lambda x: 5 * x, name='ae_reg')(x_ae)\n",
    "    \n",
    "#     model = models.Model(inputs=inp, outputs=[decoder, out_ae])\n",
    "    \n",
    "#     # Compile with adjusted loss weights\n",
    "#     model.compile(\n",
    "#         optimizer=optimizers.Adam(learning_rate=lr),\n",
    "#         loss={\n",
    "#             'decoder': losses.MeanSquaredError(),\n",
    "#             'ae_reg': losses.MeanSquaredError()\n",
    "#         },\n",
    "#         # loss_weights={\n",
    "#         #     'decoder': 0.1,  # Reduce reconstruction loss weight\n",
    "#         #     'ae_reg': 1.0    # Focus more on regression task\n",
    "#         # },\n",
    "#         metrics={\n",
    "#             'decoder': metrics.MeanAbsoluteError(name='MAE'),\n",
    "#             'ae_reg': metrics.MeanSquaredError(name='MSE')\n",
    "#         }\n",
    "#     )\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # params = {'num_columns': len(CONFIG.feature_cols), \n",
    "# #           # 'num_labels': 5, \n",
    "# #           'hidden_units': [96, 96, 896, 448, 448, 256], \n",
    "# #           'dropout_rates': [0.035, 0.038, 0.424, 0.104, 0.492, 0.320, 0.271, 0.437],\n",
    "# #           # 'ls': 0, \n",
    "# #           'lr':1e-3, \n",
    "# #          }\n",
    "\n",
    "# # Update parameters\n",
    "# params = {\n",
    "#     'num_columns': len(CONFIG.feature_cols),\n",
    "#     'hidden_units': [128, 64],  # Simplified architecture\n",
    "#     'dropout_rates': [0.01, 0.1, 0.2],  # Adjusted dropout\n",
    "#     'lr': 1e-4  # Lower learning rate\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = create_ae_mlp(**params)\n",
    "\n",
    "# ckp = callbacks.ModelCheckpoint(\n",
    "#     \"best-js-autoen.weights.h5\", \n",
    "#     monitor='val_ae_reg_MSE', \n",
    "#     verbose=0, \n",
    "#     save_best_only=True, \n",
    "#     save_weights_only=True, \n",
    "#     mode='min'\n",
    "# )\n",
    "\n",
    "# # Add learning rate scheduling\n",
    "# lr_scheduler = callbacks.ReduceLROnPlateau(\n",
    "#     monitor='val_ae_reg_MSE',\n",
    "#     factor=0.5,\n",
    "#     patience=5,\n",
    "#     min_lr=1e-6,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # Update early stopping\n",
    "# es = callbacks.EarlyStopping(\n",
    "#     monitor='val_ae_reg_MSE',\n",
    "#     min_delta=1e-4,\n",
    "#     patience=15,  # Increased patience\n",
    "#     mode='min',\n",
    "#     restore_best_weights=True,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # Train with updated callbacks\n",
    "# history = model.fit(\n",
    "#     train_generator,\n",
    "#     validation_data=valid_generator,\n",
    "#     epochs=100,\n",
    "#     callbacks=[ckp, es, lr_scheduler],\n",
    "#     verbose=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = create_ae_mlp(**params)\n",
    "# ckp = callbacks.ModelCheckpoint(\"best-js-autoen.weights.h5\", monitor = 'val_ae_reg_MSE', verbose = 0, \n",
    "#                               save_best_only = True, save_weights_only = True, mode = 'min')\n",
    "# es = callbacks.EarlyStopping(monitor = 'val_ae_reg_MSE', min_delta = 1e-4, patience = 10, mode = 'min', \n",
    "#                            baseline = None, restore_best_weights = True, verbose = 0)\n",
    "\n",
    "# history = model.fit(\n",
    "#     train_generator,\n",
    "#     validation_data=valid_generator,\n",
    "#     epochs=100,\n",
    "#     callbacks=[ckp, es],\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "#         keras.backend.clear_session()\n",
    "#         del model\n",
    "#         rubbish = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not TEST:\n",
    "#     scores = []\n",
    "#     batch_size = 4096\n",
    "#     gkf = PurgedGroupTimeSeriesSplit(n_splits = n_splits, group_gap = group_gap)\n",
    "#     for fold, (tr, te) in enumerate(gkf.split(train['action'].values, train['action'].values, train['date'].values)):\n",
    "#         ckp_path = f'JSModel_{fold}.hdf5'\n",
    "#         model = create_ae_mlp(**params)\n",
    "#         ckp = callbacks.ModelCheckpoint(ckp_path, monitor = 'val_action_AUC', verbose = 0, \n",
    "#                               save_best_only = True, save_weights_only = True, mode = 'max')\n",
    "#         es = callbacks.EarlyStopping(monitor = 'val_action_AUC', min_delta = 1e-4, patience = 10, mode = 'max', \n",
    "#                            baseline = None, restore_best_weights = True, verbose = 0)\n",
    "#         history = model.fit(X[tr], [X[tr], y[tr], y[tr]], validation_data = (X[te], [X[te], y[te], y[te]]), \n",
    "#                             sample_weight = sw[tr], \n",
    "#                             epochs = 100, batch_size = batch_size, callbacks = [ckp, es], verbose = 0)\n",
    "#         hist = pd.DataFrame(history.history)\n",
    "#         score = hist['val_action_AUC'].max()\n",
    "#         print(f'Fold {fold} ROC AUC:\\t', score)\n",
    "#         scores.append(score)\n",
    "\n",
    "#         keras.backend.clear_session()\n",
    "#         del model\n",
    "#         rubbish = gc.collect()\n",
    "    \n",
    "#     print('Weighted Average CV Score:', weighted_average(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jane_street",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
