{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/jane_street/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import os\n",
    "import pyarrow as pa\n",
    "from autogluon.tabular import TabularPredictor, TabularDataset\n",
    "from autogluon.tabular.version import __version__\n",
    "import torch\n",
    "import gc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autogluon Version: 1.2\n",
      "CUDA Available: True\n",
      "GPU Name: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "print(f\"Autogluon Version: {__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    seed = 2025\n",
    "    target_col = \"responder_6\"\n",
    "    feature_cols = [f\"feature_{idx:02d}\" for idx in range(79)] \\\n",
    "        + [f\"responder_{idx}_lag_1\" for idx in range(9)]\n",
    "    categorical_cols = []\n",
    "    train_path = \"/root/autodl-tmp/jane-street-2024/training.parquet\"\n",
    "    valid_path = \"/root/autodl-tmp/jane-street-2024/validation.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape: (25908520, 103), Validation Shape: (1341648, 103)\n"
     ]
    }
   ],
   "source": [
    "train = pl.scan_parquet(CONFIG.train_path).collect().to_pandas()\n",
    "valid = pl.scan_parquet(CONFIG.valid_path).collect().to_pandas()\n",
    "print(f\"Training Shape: {train.shape}, Validation Shape: {valid.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27250168, 103)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trick of boosting LB score, data leakage on the validation set\n",
    "train = pd.concat([train, valid]).reset_index(drop=True)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tmp = train[[\"symbol_id\", \"time_id\"] + CONFIG.feature_cols + [CONFIG.target_col] + [\"weight\"]]\n",
    "train_tmp = train_tmp.dropna(axis=1, how='all')\n",
    "train_tmp = train_tmp.loc[:, train_tmp.nunique() > 1]\n",
    "# deal with null values\n",
    "train_tmp = train_tmp.ffill().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling features\n",
    "scaler = StandardScaler()\n",
    "train_tmp[CONFIG.feature_cols] = scaler.fit_transform(train_tmp[CONFIG.feature_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>feature_00</th>\n",
       "      <th>feature_01</th>\n",
       "      <th>feature_02</th>\n",
       "      <th>feature_03</th>\n",
       "      <th>feature_04</th>\n",
       "      <th>feature_05</th>\n",
       "      <th>feature_06</th>\n",
       "      <th>feature_07</th>\n",
       "      <th>feature_08</th>\n",
       "      <th>feature_09</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "      <th>feature_25</th>\n",
       "      <th>feature_26</th>\n",
       "      <th>feature_27</th>\n",
       "      <th>feature_28</th>\n",
       "      <th>feature_29</th>\n",
       "      <th>feature_30</th>\n",
       "      <th>feature_31</th>\n",
       "      <th>feature_32</th>\n",
       "      <th>feature_33</th>\n",
       "      <th>feature_34</th>\n",
       "      <th>feature_35</th>\n",
       "      <th>feature_36</th>\n",
       "      <th>feature_37</th>\n",
       "      <th>feature_38</th>\n",
       "      <th>feature_39</th>\n",
       "      <th>feature_40</th>\n",
       "      <th>feature_41</th>\n",
       "      <th>feature_42</th>\n",
       "      <th>feature_43</th>\n",
       "      <th>feature_44</th>\n",
       "      <th>feature_45</th>\n",
       "      <th>feature_46</th>\n",
       "      <th>feature_47</th>\n",
       "      <th>feature_48</th>\n",
       "      <th>feature_49</th>\n",
       "      <th>feature_50</th>\n",
       "      <th>feature_51</th>\n",
       "      <th>feature_52</th>\n",
       "      <th>feature_53</th>\n",
       "      <th>feature_54</th>\n",
       "      <th>feature_55</th>\n",
       "      <th>feature_56</th>\n",
       "      <th>feature_57</th>\n",
       "      <th>feature_58</th>\n",
       "      <th>feature_59</th>\n",
       "      <th>feature_60</th>\n",
       "      <th>feature_61</th>\n",
       "      <th>feature_62</th>\n",
       "      <th>feature_63</th>\n",
       "      <th>feature_64</th>\n",
       "      <th>feature_65</th>\n",
       "      <th>feature_66</th>\n",
       "      <th>feature_67</th>\n",
       "      <th>feature_68</th>\n",
       "      <th>feature_69</th>\n",
       "      <th>feature_70</th>\n",
       "      <th>feature_71</th>\n",
       "      <th>feature_72</th>\n",
       "      <th>feature_73</th>\n",
       "      <th>feature_74</th>\n",
       "      <th>feature_75</th>\n",
       "      <th>feature_76</th>\n",
       "      <th>feature_77</th>\n",
       "      <th>feature_78</th>\n",
       "      <th>responder_0_lag_1</th>\n",
       "      <th>responder_1_lag_1</th>\n",
       "      <th>responder_2_lag_1</th>\n",
       "      <th>responder_3_lag_1</th>\n",
       "      <th>responder_4_lag_1</th>\n",
       "      <th>responder_5_lag_1</th>\n",
       "      <th>responder_6_lag_1</th>\n",
       "      <th>responder_7_lag_1</th>\n",
       "      <th>responder_8_lag_1</th>\n",
       "      <th>responder_6</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.896643</td>\n",
       "      <td>-0.610005</td>\n",
       "      <td>-0.995681</td>\n",
       "      <td>-0.954077</td>\n",
       "      <td>-2.760670</td>\n",
       "      <td>0.288205</td>\n",
       "      <td>1.873081</td>\n",
       "      <td>0.094930</td>\n",
       "      <td>-0.392002</td>\n",
       "      <td>-0.916119</td>\n",
       "      <td>0.645358</td>\n",
       "      <td>-0.558358</td>\n",
       "      <td>-0.687803</td>\n",
       "      <td>0.591037</td>\n",
       "      <td>-0.324128</td>\n",
       "      <td>0.631278</td>\n",
       "      <td>-0.167471</td>\n",
       "      <td>0.603672</td>\n",
       "      <td>-1.993188</td>\n",
       "      <td>-1.127251</td>\n",
       "      <td>1.023338</td>\n",
       "      <td>-0.565472</td>\n",
       "      <td>0.783994</td>\n",
       "      <td>0.714467</td>\n",
       "      <td>-0.454944</td>\n",
       "      <td>-0.406669</td>\n",
       "      <td>1.024027</td>\n",
       "      <td>2.409864</td>\n",
       "      <td>0.918978</td>\n",
       "      <td>0.266725</td>\n",
       "      <td>0.138662</td>\n",
       "      <td>-0.768144</td>\n",
       "      <td>-0.565228</td>\n",
       "      <td>0.031462</td>\n",
       "      <td>-1.699263</td>\n",
       "      <td>-1.820564</td>\n",
       "      <td>-1.942219</td>\n",
       "      <td>-0.165838</td>\n",
       "      <td>-0.594687</td>\n",
       "      <td>-0.077806</td>\n",
       "      <td>2.282816</td>\n",
       "      <td>-0.110153</td>\n",
       "      <td>0.143219</td>\n",
       "      <td>-0.302822</td>\n",
       "      <td>0.20446</td>\n",
       "      <td>-1.748215</td>\n",
       "      <td>1.440617</td>\n",
       "      <td>0.900254</td>\n",
       "      <td>0.359193</td>\n",
       "      <td>0.432366</td>\n",
       "      <td>0.118912</td>\n",
       "      <td>2.849400</td>\n",
       "      <td>0.198372</td>\n",
       "      <td>-0.131024</td>\n",
       "      <td>-1.300356</td>\n",
       "      <td>-0.167255</td>\n",
       "      <td>-0.938577</td>\n",
       "      <td>1.329146</td>\n",
       "      <td>0.030262</td>\n",
       "      <td>3.077584</td>\n",
       "      <td>1.163163</td>\n",
       "      <td>1.223827</td>\n",
       "      <td>0.096157</td>\n",
       "      <td>0.536366</td>\n",
       "      <td>0.096771</td>\n",
       "      <td>-1.450948</td>\n",
       "      <td>-1.371611</td>\n",
       "      <td>-0.826808</td>\n",
       "      <td>0.351071</td>\n",
       "      <td>-0.327754</td>\n",
       "      <td>-1.158452</td>\n",
       "      <td>1.113587</td>\n",
       "      <td>-0.564873</td>\n",
       "      <td>0.029608</td>\n",
       "      <td>0.043557</td>\n",
       "      <td>-0.223579</td>\n",
       "      <td>-0.256480</td>\n",
       "      <td>-0.345467</td>\n",
       "      <td>-0.327794</td>\n",
       "      <td>-0.445440</td>\n",
       "      <td>-1.046363</td>\n",
       "      <td>-0.536402</td>\n",
       "      <td>0.045927</td>\n",
       "      <td>-0.016934</td>\n",
       "      <td>0.205160</td>\n",
       "      <td>0.878791</td>\n",
       "      <td>1.004436</td>\n",
       "      <td>0.892570</td>\n",
       "      <td>0.496563</td>\n",
       "      <td>3.324375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.002535</td>\n",
       "      <td>-0.706006</td>\n",
       "      <td>-1.017293</td>\n",
       "      <td>-1.153922</td>\n",
       "      <td>-2.469186</td>\n",
       "      <td>0.271089</td>\n",
       "      <td>2.589536</td>\n",
       "      <td>0.076709</td>\n",
       "      <td>-0.523450</td>\n",
       "      <td>-0.916119</td>\n",
       "      <td>0.645358</td>\n",
       "      <td>-0.558358</td>\n",
       "      <td>-1.149239</td>\n",
       "      <td>0.287937</td>\n",
       "      <td>-0.453126</td>\n",
       "      <td>0.631278</td>\n",
       "      <td>-0.480747</td>\n",
       "      <td>0.603672</td>\n",
       "      <td>-2.014348</td>\n",
       "      <td>-1.252671</td>\n",
       "      <td>0.892914</td>\n",
       "      <td>-0.116104</td>\n",
       "      <td>2.042073</td>\n",
       "      <td>1.164534</td>\n",
       "      <td>-0.395741</td>\n",
       "      <td>-0.178578</td>\n",
       "      <td>0.454699</td>\n",
       "      <td>1.535051</td>\n",
       "      <td>1.708965</td>\n",
       "      <td>-0.273597</td>\n",
       "      <td>-0.323041</td>\n",
       "      <td>-0.113742</td>\n",
       "      <td>-0.565228</td>\n",
       "      <td>0.031462</td>\n",
       "      <td>-0.905948</td>\n",
       "      <td>-1.255449</td>\n",
       "      <td>0.216441</td>\n",
       "      <td>-0.202460</td>\n",
       "      <td>-0.432976</td>\n",
       "      <td>-0.077806</td>\n",
       "      <td>1.016129</td>\n",
       "      <td>-0.110153</td>\n",
       "      <td>0.143219</td>\n",
       "      <td>-0.705592</td>\n",
       "      <td>0.20446</td>\n",
       "      <td>-1.786147</td>\n",
       "      <td>1.418459</td>\n",
       "      <td>0.320197</td>\n",
       "      <td>0.790277</td>\n",
       "      <td>0.576228</td>\n",
       "      <td>0.118912</td>\n",
       "      <td>1.954982</td>\n",
       "      <td>0.198372</td>\n",
       "      <td>-0.131024</td>\n",
       "      <td>1.166803</td>\n",
       "      <td>-0.167255</td>\n",
       "      <td>-1.794623</td>\n",
       "      <td>2.600993</td>\n",
       "      <td>0.030262</td>\n",
       "      <td>1.540443</td>\n",
       "      <td>1.029736</td>\n",
       "      <td>1.223827</td>\n",
       "      <td>-0.122236</td>\n",
       "      <td>-0.557287</td>\n",
       "      <td>-0.424072</td>\n",
       "      <td>-2.040446</td>\n",
       "      <td>-1.481852</td>\n",
       "      <td>-0.357632</td>\n",
       "      <td>0.239100</td>\n",
       "      <td>-0.526793</td>\n",
       "      <td>-0.822562</td>\n",
       "      <td>0.246273</td>\n",
       "      <td>-0.676566</td>\n",
       "      <td>0.029608</td>\n",
       "      <td>0.043557</td>\n",
       "      <td>-0.318110</td>\n",
       "      <td>-0.247092</td>\n",
       "      <td>-0.345120</td>\n",
       "      <td>-0.310614</td>\n",
       "      <td>-0.360236</td>\n",
       "      <td>0.069464</td>\n",
       "      <td>-0.426800</td>\n",
       "      <td>-0.265364</td>\n",
       "      <td>-0.233734</td>\n",
       "      <td>-0.595318</td>\n",
       "      <td>-1.540195</td>\n",
       "      <td>-1.183527</td>\n",
       "      <td>-0.891440</td>\n",
       "      <td>0.529877</td>\n",
       "      <td>4.711303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.230939</td>\n",
       "      <td>-1.120955</td>\n",
       "      <td>-1.031138</td>\n",
       "      <td>-1.084398</td>\n",
       "      <td>-2.506928</td>\n",
       "      <td>0.342476</td>\n",
       "      <td>2.920083</td>\n",
       "      <td>0.121616</td>\n",
       "      <td>-0.815010</td>\n",
       "      <td>2.074254</td>\n",
       "      <td>-0.928510</td>\n",
       "      <td>-0.663494</td>\n",
       "      <td>-0.876131</td>\n",
       "      <td>1.827359</td>\n",
       "      <td>-0.235610</td>\n",
       "      <td>0.631278</td>\n",
       "      <td>0.201888</td>\n",
       "      <td>0.603672</td>\n",
       "      <td>-1.719610</td>\n",
       "      <td>-1.829798</td>\n",
       "      <td>-1.112130</td>\n",
       "      <td>-0.581926</td>\n",
       "      <td>0.765638</td>\n",
       "      <td>0.274209</td>\n",
       "      <td>-1.270422</td>\n",
       "      <td>-1.140752</td>\n",
       "      <td>0.558969</td>\n",
       "      <td>1.795410</td>\n",
       "      <td>1.370559</td>\n",
       "      <td>-0.462291</td>\n",
       "      <td>0.070380</td>\n",
       "      <td>-0.360759</td>\n",
       "      <td>-0.565228</td>\n",
       "      <td>0.031462</td>\n",
       "      <td>-1.156181</td>\n",
       "      <td>-1.632147</td>\n",
       "      <td>-1.834713</td>\n",
       "      <td>0.173300</td>\n",
       "      <td>-0.021549</td>\n",
       "      <td>-0.077806</td>\n",
       "      <td>1.397663</td>\n",
       "      <td>-0.110153</td>\n",
       "      <td>0.143219</td>\n",
       "      <td>-2.071595</td>\n",
       "      <td>0.20446</td>\n",
       "      <td>-1.989080</td>\n",
       "      <td>1.013458</td>\n",
       "      <td>-0.467515</td>\n",
       "      <td>-1.626455</td>\n",
       "      <td>-2.462493</td>\n",
       "      <td>0.118912</td>\n",
       "      <td>1.652670</td>\n",
       "      <td>0.198372</td>\n",
       "      <td>-0.131024</td>\n",
       "      <td>-2.159296</td>\n",
       "      <td>-0.167255</td>\n",
       "      <td>-1.459922</td>\n",
       "      <td>1.221402</td>\n",
       "      <td>0.030262</td>\n",
       "      <td>-2.014698</td>\n",
       "      <td>-0.620301</td>\n",
       "      <td>1.223827</td>\n",
       "      <td>0.118930</td>\n",
       "      <td>0.222961</td>\n",
       "      <td>-0.163160</td>\n",
       "      <td>-1.637916</td>\n",
       "      <td>-1.879726</td>\n",
       "      <td>-1.109044</td>\n",
       "      <td>1.527212</td>\n",
       "      <td>-0.166839</td>\n",
       "      <td>-0.891068</td>\n",
       "      <td>1.303491</td>\n",
       "      <td>-0.307766</td>\n",
       "      <td>0.029608</td>\n",
       "      <td>0.043557</td>\n",
       "      <td>0.736153</td>\n",
       "      <td>0.489172</td>\n",
       "      <td>-0.058928</td>\n",
       "      <td>-0.042220</td>\n",
       "      <td>-0.382783</td>\n",
       "      <td>0.094874</td>\n",
       "      <td>-0.399741</td>\n",
       "      <td>-0.145459</td>\n",
       "      <td>-0.121599</td>\n",
       "      <td>0.207478</td>\n",
       "      <td>0.440513</td>\n",
       "      <td>0.418602</td>\n",
       "      <td>0.230630</td>\n",
       "      <td>0.746983</td>\n",
       "      <td>3.028847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.225174</td>\n",
       "      <td>-0.253832</td>\n",
       "      <td>-1.083930</td>\n",
       "      <td>-0.742088</td>\n",
       "      <td>-2.597986</td>\n",
       "      <td>0.218405</td>\n",
       "      <td>1.927774</td>\n",
       "      <td>0.069894</td>\n",
       "      <td>-0.381086</td>\n",
       "      <td>-1.215156</td>\n",
       "      <td>-0.613736</td>\n",
       "      <td>-0.960352</td>\n",
       "      <td>-1.061723</td>\n",
       "      <td>0.966756</td>\n",
       "      <td>-0.604590</td>\n",
       "      <td>0.631278</td>\n",
       "      <td>0.085860</td>\n",
       "      <td>0.603672</td>\n",
       "      <td>-1.811111</td>\n",
       "      <td>-1.983444</td>\n",
       "      <td>0.132673</td>\n",
       "      <td>0.987362</td>\n",
       "      <td>0.348089</td>\n",
       "      <td>-0.285520</td>\n",
       "      <td>1.080590</td>\n",
       "      <td>0.640839</td>\n",
       "      <td>-0.334247</td>\n",
       "      <td>-0.211743</td>\n",
       "      <td>-0.136426</td>\n",
       "      <td>-0.084064</td>\n",
       "      <td>-0.852502</td>\n",
       "      <td>0.905843</td>\n",
       "      <td>-0.565228</td>\n",
       "      <td>0.031462</td>\n",
       "      <td>-1.041331</td>\n",
       "      <td>-0.716300</td>\n",
       "      <td>-1.235662</td>\n",
       "      <td>-0.229454</td>\n",
       "      <td>-0.292190</td>\n",
       "      <td>-0.077806</td>\n",
       "      <td>1.570834</td>\n",
       "      <td>-0.110153</td>\n",
       "      <td>0.143219</td>\n",
       "      <td>0.317223</td>\n",
       "      <td>0.20446</td>\n",
       "      <td>-0.813999</td>\n",
       "      <td>1.339600</td>\n",
       "      <td>0.835345</td>\n",
       "      <td>3.895741</td>\n",
       "      <td>1.408002</td>\n",
       "      <td>0.118912</td>\n",
       "      <td>3.170727</td>\n",
       "      <td>0.198372</td>\n",
       "      <td>-0.131024</td>\n",
       "      <td>-0.101458</td>\n",
       "      <td>-0.167255</td>\n",
       "      <td>-0.829015</td>\n",
       "      <td>1.733263</td>\n",
       "      <td>0.030262</td>\n",
       "      <td>4.816560</td>\n",
       "      <td>1.746597</td>\n",
       "      <td>1.223827</td>\n",
       "      <td>-0.075761</td>\n",
       "      <td>0.771788</td>\n",
       "      <td>0.405967</td>\n",
       "      <td>-1.179221</td>\n",
       "      <td>-1.441056</td>\n",
       "      <td>-0.858955</td>\n",
       "      <td>0.903640</td>\n",
       "      <td>-0.408190</td>\n",
       "      <td>-1.124283</td>\n",
       "      <td>0.605557</td>\n",
       "      <td>-0.644534</td>\n",
       "      <td>0.029608</td>\n",
       "      <td>0.043557</td>\n",
       "      <td>3.529042</td>\n",
       "      <td>3.995298</td>\n",
       "      <td>0.624213</td>\n",
       "      <td>0.734843</td>\n",
       "      <td>3.654261</td>\n",
       "      <td>0.387400</td>\n",
       "      <td>0.611840</td>\n",
       "      <td>0.114229</td>\n",
       "      <td>0.128016</td>\n",
       "      <td>-0.016035</td>\n",
       "      <td>-0.353038</td>\n",
       "      <td>-0.390723</td>\n",
       "      <td>-0.383279</td>\n",
       "      <td>0.941218</td>\n",
       "      <td>2.099438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.972385</td>\n",
       "      <td>-0.344859</td>\n",
       "      <td>-1.172583</td>\n",
       "      <td>-1.072821</td>\n",
       "      <td>-2.549062</td>\n",
       "      <td>0.139879</td>\n",
       "      <td>0.962981</td>\n",
       "      <td>0.053145</td>\n",
       "      <td>-0.361565</td>\n",
       "      <td>-0.745240</td>\n",
       "      <td>-1.243284</td>\n",
       "      <td>-0.972721</td>\n",
       "      <td>-0.776210</td>\n",
       "      <td>0.169685</td>\n",
       "      <td>-0.652229</td>\n",
       "      <td>0.631278</td>\n",
       "      <td>0.065304</td>\n",
       "      <td>0.603672</td>\n",
       "      <td>-2.840220</td>\n",
       "      <td>-1.003263</td>\n",
       "      <td>-0.387320</td>\n",
       "      <td>1.272878</td>\n",
       "      <td>0.735418</td>\n",
       "      <td>-0.624473</td>\n",
       "      <td>3.802739</td>\n",
       "      <td>2.052129</td>\n",
       "      <td>-1.126798</td>\n",
       "      <td>-0.037140</td>\n",
       "      <td>0.769159</td>\n",
       "      <td>-0.176020</td>\n",
       "      <td>-0.290959</td>\n",
       "      <td>1.750240</td>\n",
       "      <td>-0.565228</td>\n",
       "      <td>0.031462</td>\n",
       "      <td>-1.026004</td>\n",
       "      <td>-2.075983</td>\n",
       "      <td>-3.734874</td>\n",
       "      <td>-0.177810</td>\n",
       "      <td>-0.386510</td>\n",
       "      <td>-0.077806</td>\n",
       "      <td>1.294092</td>\n",
       "      <td>-0.110153</td>\n",
       "      <td>0.143219</td>\n",
       "      <td>-0.748033</td>\n",
       "      <td>0.20446</td>\n",
       "      <td>-1.273917</td>\n",
       "      <td>1.744212</td>\n",
       "      <td>0.062925</td>\n",
       "      <td>0.743748</td>\n",
       "      <td>-0.095948</td>\n",
       "      <td>0.118912</td>\n",
       "      <td>2.894469</td>\n",
       "      <td>0.198372</td>\n",
       "      <td>-0.131024</td>\n",
       "      <td>-0.485311</td>\n",
       "      <td>-0.167255</td>\n",
       "      <td>-1.320898</td>\n",
       "      <td>1.955807</td>\n",
       "      <td>0.030262</td>\n",
       "      <td>3.437934</td>\n",
       "      <td>1.641095</td>\n",
       "      <td>1.223827</td>\n",
       "      <td>-0.010995</td>\n",
       "      <td>1.094658</td>\n",
       "      <td>0.078890</td>\n",
       "      <td>-1.341557</td>\n",
       "      <td>-1.309457</td>\n",
       "      <td>-1.273147</td>\n",
       "      <td>0.126753</td>\n",
       "      <td>-0.543311</td>\n",
       "      <td>-1.151026</td>\n",
       "      <td>0.139384</td>\n",
       "      <td>-0.642059</td>\n",
       "      <td>0.029608</td>\n",
       "      <td>0.043557</td>\n",
       "      <td>5.592292</td>\n",
       "      <td>4.160807</td>\n",
       "      <td>1.217082</td>\n",
       "      <td>0.683053</td>\n",
       "      <td>-0.124827</td>\n",
       "      <td>-0.120259</td>\n",
       "      <td>0.344871</td>\n",
       "      <td>-0.162151</td>\n",
       "      <td>-0.127740</td>\n",
       "      <td>0.047159</td>\n",
       "      <td>-0.523502</td>\n",
       "      <td>-0.452244</td>\n",
       "      <td>-0.441841</td>\n",
       "      <td>0.204584</td>\n",
       "      <td>3.166049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   symbol_id  time_id  feature_00  feature_01  feature_02  feature_03  \\\n",
       "0          0        0   -0.896643   -0.610005   -0.995681   -0.954077   \n",
       "1          1        0   -1.002535   -0.706006   -1.017293   -1.153922   \n",
       "2          2        0   -1.230939   -1.120955   -1.031138   -1.084398   \n",
       "3          3        0   -1.225174   -0.253832   -1.083930   -0.742088   \n",
       "4          4        0   -0.972385   -0.344859   -1.172583   -1.072821   \n",
       "\n",
       "   feature_04  feature_05  feature_06  feature_07  feature_08  feature_09  \\\n",
       "0   -2.760670    0.288205    1.873081    0.094930   -0.392002   -0.916119   \n",
       "1   -2.469186    0.271089    2.589536    0.076709   -0.523450   -0.916119   \n",
       "2   -2.506928    0.342476    2.920083    0.121616   -0.815010    2.074254   \n",
       "3   -2.597986    0.218405    1.927774    0.069894   -0.381086   -1.215156   \n",
       "4   -2.549062    0.139879    0.962981    0.053145   -0.361565   -0.745240   \n",
       "\n",
       "   feature_10  feature_11  feature_12  feature_13  feature_14  feature_15  \\\n",
       "0    0.645358   -0.558358   -0.687803    0.591037   -0.324128    0.631278   \n",
       "1    0.645358   -0.558358   -1.149239    0.287937   -0.453126    0.631278   \n",
       "2   -0.928510   -0.663494   -0.876131    1.827359   -0.235610    0.631278   \n",
       "3   -0.613736   -0.960352   -1.061723    0.966756   -0.604590    0.631278   \n",
       "4   -1.243284   -0.972721   -0.776210    0.169685   -0.652229    0.631278   \n",
       "\n",
       "   feature_16  feature_17  feature_18  feature_19  feature_20  feature_21  \\\n",
       "0   -0.167471    0.603672   -1.993188   -1.127251    1.023338   -0.565472   \n",
       "1   -0.480747    0.603672   -2.014348   -1.252671    0.892914   -0.116104   \n",
       "2    0.201888    0.603672   -1.719610   -1.829798   -1.112130   -0.581926   \n",
       "3    0.085860    0.603672   -1.811111   -1.983444    0.132673    0.987362   \n",
       "4    0.065304    0.603672   -2.840220   -1.003263   -0.387320    1.272878   \n",
       "\n",
       "   feature_22  feature_23  feature_24  feature_25  feature_26  feature_27  \\\n",
       "0    0.783994    0.714467   -0.454944   -0.406669    1.024027    2.409864   \n",
       "1    2.042073    1.164534   -0.395741   -0.178578    0.454699    1.535051   \n",
       "2    0.765638    0.274209   -1.270422   -1.140752    0.558969    1.795410   \n",
       "3    0.348089   -0.285520    1.080590    0.640839   -0.334247   -0.211743   \n",
       "4    0.735418   -0.624473    3.802739    2.052129   -1.126798   -0.037140   \n",
       "\n",
       "   feature_28  feature_29  feature_30  feature_31  feature_32  feature_33  \\\n",
       "0    0.918978    0.266725    0.138662   -0.768144   -0.565228    0.031462   \n",
       "1    1.708965   -0.273597   -0.323041   -0.113742   -0.565228    0.031462   \n",
       "2    1.370559   -0.462291    0.070380   -0.360759   -0.565228    0.031462   \n",
       "3   -0.136426   -0.084064   -0.852502    0.905843   -0.565228    0.031462   \n",
       "4    0.769159   -0.176020   -0.290959    1.750240   -0.565228    0.031462   \n",
       "\n",
       "   feature_34  feature_35  feature_36  feature_37  feature_38  feature_39  \\\n",
       "0   -1.699263   -1.820564   -1.942219   -0.165838   -0.594687   -0.077806   \n",
       "1   -0.905948   -1.255449    0.216441   -0.202460   -0.432976   -0.077806   \n",
       "2   -1.156181   -1.632147   -1.834713    0.173300   -0.021549   -0.077806   \n",
       "3   -1.041331   -0.716300   -1.235662   -0.229454   -0.292190   -0.077806   \n",
       "4   -1.026004   -2.075983   -3.734874   -0.177810   -0.386510   -0.077806   \n",
       "\n",
       "   feature_40  feature_41  feature_42  feature_43  feature_44  feature_45  \\\n",
       "0    2.282816   -0.110153    0.143219   -0.302822     0.20446   -1.748215   \n",
       "1    1.016129   -0.110153    0.143219   -0.705592     0.20446   -1.786147   \n",
       "2    1.397663   -0.110153    0.143219   -2.071595     0.20446   -1.989080   \n",
       "3    1.570834   -0.110153    0.143219    0.317223     0.20446   -0.813999   \n",
       "4    1.294092   -0.110153    0.143219   -0.748033     0.20446   -1.273917   \n",
       "\n",
       "   feature_46  feature_47  feature_48  feature_49  feature_50  feature_51  \\\n",
       "0    1.440617    0.900254    0.359193    0.432366    0.118912    2.849400   \n",
       "1    1.418459    0.320197    0.790277    0.576228    0.118912    1.954982   \n",
       "2    1.013458   -0.467515   -1.626455   -2.462493    0.118912    1.652670   \n",
       "3    1.339600    0.835345    3.895741    1.408002    0.118912    3.170727   \n",
       "4    1.744212    0.062925    0.743748   -0.095948    0.118912    2.894469   \n",
       "\n",
       "   feature_52  feature_53  feature_54  feature_55  feature_56  feature_57  \\\n",
       "0    0.198372   -0.131024   -1.300356   -0.167255   -0.938577    1.329146   \n",
       "1    0.198372   -0.131024    1.166803   -0.167255   -1.794623    2.600993   \n",
       "2    0.198372   -0.131024   -2.159296   -0.167255   -1.459922    1.221402   \n",
       "3    0.198372   -0.131024   -0.101458   -0.167255   -0.829015    1.733263   \n",
       "4    0.198372   -0.131024   -0.485311   -0.167255   -1.320898    1.955807   \n",
       "\n",
       "   feature_58  feature_59  feature_60  feature_61  feature_62  feature_63  \\\n",
       "0    0.030262    3.077584    1.163163    1.223827    0.096157    0.536366   \n",
       "1    0.030262    1.540443    1.029736    1.223827   -0.122236   -0.557287   \n",
       "2    0.030262   -2.014698   -0.620301    1.223827    0.118930    0.222961   \n",
       "3    0.030262    4.816560    1.746597    1.223827   -0.075761    0.771788   \n",
       "4    0.030262    3.437934    1.641095    1.223827   -0.010995    1.094658   \n",
       "\n",
       "   feature_64  feature_65  feature_66  feature_67  feature_68  feature_69  \\\n",
       "0    0.096771   -1.450948   -1.371611   -0.826808    0.351071   -0.327754   \n",
       "1   -0.424072   -2.040446   -1.481852   -0.357632    0.239100   -0.526793   \n",
       "2   -0.163160   -1.637916   -1.879726   -1.109044    1.527212   -0.166839   \n",
       "3    0.405967   -1.179221   -1.441056   -0.858955    0.903640   -0.408190   \n",
       "4    0.078890   -1.341557   -1.309457   -1.273147    0.126753   -0.543311   \n",
       "\n",
       "   feature_70  feature_71  feature_72  feature_73  feature_74  feature_75  \\\n",
       "0   -1.158452    1.113587   -0.564873    0.029608    0.043557   -0.223579   \n",
       "1   -0.822562    0.246273   -0.676566    0.029608    0.043557   -0.318110   \n",
       "2   -0.891068    1.303491   -0.307766    0.029608    0.043557    0.736153   \n",
       "3   -1.124283    0.605557   -0.644534    0.029608    0.043557    3.529042   \n",
       "4   -1.151026    0.139384   -0.642059    0.029608    0.043557    5.592292   \n",
       "\n",
       "   feature_76  feature_77  feature_78  responder_0_lag_1  responder_1_lag_1  \\\n",
       "0   -0.256480   -0.345467   -0.327794          -0.445440          -1.046363   \n",
       "1   -0.247092   -0.345120   -0.310614          -0.360236           0.069464   \n",
       "2    0.489172   -0.058928   -0.042220          -0.382783           0.094874   \n",
       "3    3.995298    0.624213    0.734843           3.654261           0.387400   \n",
       "4    4.160807    1.217082    0.683053          -0.124827          -0.120259   \n",
       "\n",
       "   responder_2_lag_1  responder_3_lag_1  responder_4_lag_1  responder_5_lag_1  \\\n",
       "0          -0.536402           0.045927          -0.016934           0.205160   \n",
       "1          -0.426800          -0.265364          -0.233734          -0.595318   \n",
       "2          -0.399741          -0.145459          -0.121599           0.207478   \n",
       "3           0.611840           0.114229           0.128016          -0.016035   \n",
       "4           0.344871          -0.162151          -0.127740           0.047159   \n",
       "\n",
       "   responder_6_lag_1  responder_7_lag_1  responder_8_lag_1  responder_6  \\\n",
       "0           0.878791           1.004436           0.892570     0.496563   \n",
       "1          -1.540195          -1.183527          -0.891440     0.529877   \n",
       "2           0.440513           0.418602           0.230630     0.746983   \n",
       "3          -0.353038          -0.390723          -0.383279     0.941218   \n",
       "4          -0.523502          -0.452244          -0.441841     0.204584   \n",
       "\n",
       "     weight  \n",
       "0  3.324375  \n",
       "1  4.711303  \n",
       "2  3.028847  \n",
       "3  2.099438  \n",
       "4  3.166049  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_automd = TabularDataset(train_tmp)\n",
    "train_automd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Autogluon Predictor Configuration\n",
    "predictor_kwargs = {\n",
    "    'label': CONFIG.target_col,\n",
    "    'eval_metric': \"r2\",\n",
    "    'sample_weight': \"weight\",\n",
    "    'weight_evaluation': True,\n",
    "    'problem_type': 'regression',\n",
    "    'verbosity': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_kwargs = {\n",
    "    'train_data': train_automd,\n",
    "    'presets': \"medium_quality\",\n",
    "    'time_limit': 3600*9,  # 9 hours\n",
    "    'ag_args_fit': {\n",
    "        'num_gpus': 1\n",
    "    },\n",
    "    'excluded_model_types': ['RF', 'XT', 'KNN', 'GBM', 'FASTAI', 'CAT'],\n",
    "    # 'num_stack_levels': 1,\n",
    "    # 'num_bag_folds': 3,\n",
    "    'hyperparameters': {\n",
    "        'XGB': {\n",
    "            'extra_trees': True,\n",
    "            'ag_args': {\n",
    "                'name_suffix': 'XGB',\n",
    "                'use_gpu': True,\n",
    "            },\n",
    "        },\n",
    "        'NeuralNet': {\n",
    "            'epochs': 50,\n",
    "            'learning_rate': 1e-3,\n",
    "            'ag_args': {\n",
    "                'name_suffix': 'NN',\n",
    "                'use_gpu': True,\n",
    "            },\n",
    "        },\n",
    "        # Add more models or tune specific models as needed\n",
    "    },\n",
    "    # 'hyperparameter_tune_kwargs': {\n",
    "    #     'scheduler': 'local',\n",
    "    #     'searcher': 'random',\n",
    "    #     'num_trials': 20,\n",
    "    # },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20241221_105141\"\n",
      "Verbosity: 4 (Maximum Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.10.16\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #147-Ubuntu SMP Fri Oct 14 17:07:22 UTC 2022\n",
      "CPU Count:          128\n",
      "GPU Count:          1\n",
      "Memory Avail:       690.06 GB / 755.30 GB (91.4%)\n",
      "Disk Space Avail:   31.79 GB / 50.00 GB (63.6%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "============ fit kwarg info ============\n",
      "User Specified kwargs:\n",
      "{'ag_args_fit': {'num_gpus': 1},\n",
      " 'auto_stack': False,\n",
      " 'excluded_model_types': ['RF', 'XT', 'KNN', 'GBM', 'FASTAI', 'CAT']}\n",
      "Full kwargs:\n",
      "{'_feature_generator_kwargs': None,\n",
      " '_save_bag_folds': None,\n",
      " 'ag_args': None,\n",
      " 'ag_args_ensemble': None,\n",
      " 'ag_args_fit': {'num_gpus': 1},\n",
      " 'auto_stack': False,\n",
      " 'calibrate': 'auto',\n",
      " 'delay_bag_sets': False,\n",
      " 'ds_args': {'clean_up_fits': True,\n",
      "             'detection_time_frac': 0.25,\n",
      "             'enable_callbacks': False,\n",
      "             'enable_ray_logging': True,\n",
      "             'holdout_data': None,\n",
      "             'holdout_frac': 0.1111111111111111,\n",
      "             'memory_safe_fits': True,\n",
      "             'n_folds': 2,\n",
      "             'n_repeats': 1,\n",
      "             'validation_procedure': 'holdout'},\n",
      " 'excluded_model_types': ['RF', 'XT', 'KNN', 'GBM', 'FASTAI', 'CAT'],\n",
      " 'feature_generator': 'auto',\n",
      " 'feature_prune_kwargs': None,\n",
      " 'holdout_frac': None,\n",
      " 'hyperparameter_tune_kwargs': None,\n",
      " 'included_model_types': None,\n",
      " 'keep_only_best': False,\n",
      " 'learning_curves': False,\n",
      " 'name_suffix': None,\n",
      " 'num_bag_folds': None,\n",
      " 'num_bag_sets': None,\n",
      " 'num_stack_levels': None,\n",
      " 'pseudo_data': None,\n",
      " 'raise_on_no_models_fitted': True,\n",
      " 'refit_full': False,\n",
      " 'save_bag_folds': None,\n",
      " 'save_space': False,\n",
      " 'set_best_to_refit_full': False,\n",
      " 'test_data': None,\n",
      " 'unlabeled_data': None,\n",
      " 'use_bag_holdout': False,\n",
      " 'verbosity': 4}\n",
      "========================================\n",
      "Saving /root/autodl-tmp/jane-street-2024/AutogluonModels/ag-20241221_105141/learner.pkl\n",
      "Saving /root/autodl-tmp/jane-street-2024/AutogluonModels/ag-20241221_105141/predictor.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Values in column 'weight' used as sample weights instead of predictive features. Evaluation will report weighted metrics, so ensure same column exists in test data.\n",
      "Beginning AutoGluon training ... Time limit = 32400s\n",
      "AutoGluon will save models to \"/root/autodl-tmp/jane-street-2024/AutogluonModels/ag-20241221_105141\"\n",
      "Train Data Rows:    27250168\n",
      "Train Data Columns: 91\n",
      "Label Column:       responder_6\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    706188.23 MB\n",
      "\tTrain Data (Original)  Memory Usage: 9225.66 MB (1.3% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tOriginal Features (exact raw dtype, raw dtype):\n",
      "\t\t\t\t('float32', 'float') : 88 | ['feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', ...]\n",
      "\t\t\t\t('int16', 'int')     :  1 | ['time_id']\n",
      "\t\t\t\t('int8', 'int')      :  1 | ['symbol_id']\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 88 | ['feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', ...]\n",
      "\t\t\t\t('int', [])   :  2 | ['symbol_id', 'time_id']\n",
      "\t\t\tTypes of features in processed data (exact raw dtype, raw dtype):\n",
      "\t\t\t\t('float32', 'float') : 88 | ['feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', ...]\n",
      "\t\t\t\t('int16', 'int')     :  1 | ['time_id']\n",
      "\t\t\t\t('int8', 'int')      :  1 | ['symbol_id']\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 88 | ['feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', ...]\n",
      "\t\t\t\t('int', [])   :  2 | ['symbol_id', 'time_id']\n",
      "\t\t\t72.1s = Fit runtime\n",
      "\t\t\t90 features in original data used to generate 90 features in processed data.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 88 | ['feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', ...]\n",
      "\t\t\t\t('int', [])   :  2 | ['symbol_id', 'time_id']\n",
      "\t\t\tTypes of features in processed data (exact raw dtype, raw dtype):\n",
      "\t\t\t\t('float32', 'float') : 88 | ['feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', ...]\n",
      "\t\t\t\t('int16', 'int')     :  1 | ['time_id']\n",
      "\t\t\t\t('int8', 'int')      :  1 | ['symbol_id']\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 88 | ['feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', ...]\n",
      "\t\t\t\t('int', [])   :  2 | ['symbol_id', 'time_id']\n",
      "\t\t\t10.7s = Fit runtime\n",
      "\t\t\t90 features in original data used to generate 90 features in processed data.\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 88 | ['feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', ...]\n",
      "\t\t\t\t('int', [])   :  2 | ['symbol_id', 'time_id']\n",
      "\t\t\tTypes of features in processed data (exact raw dtype, raw dtype):\n",
      "\t\t\t\t('float32', 'float') : 88 | ['feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', ...]\n",
      "\t\t\t\t('int16', 'int')     :  1 | ['time_id']\n",
      "\t\t\t\t('int8', 'int')      :  1 | ['symbol_id']\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 88 | ['feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', ...]\n",
      "\t\t\t\t('int', [])   :  2 | ['symbol_id', 'time_id']\n",
      "\t\t\t10.5s = Fit runtime\n",
      "\t\t\t90 features in original data used to generate 90 features in processed data.\n",
      "\t\tSkipping CategoryFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping DatetimeFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping TextSpecialFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping TextNgramFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IdentityFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IsNanFeatureGenerator: No input feature with required dtypes.\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    }
   ],
   "source": [
    "# train using AutoGluon\n",
    "predictor = TabularPredictor(**predictor_kwargs)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "predictor.fit(**train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for autogluon results displaying\n",
    "predictor.leaderboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JaneStreet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
