{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/jane_street/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import os\n",
    "import pyarrow as pa\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    seeds = [2025, 42, 44, 100] # 每折用不同的seed\n",
    "    target_col = \"responder_6\"\n",
    "    feature_cols = [\"symbol_id\"] \\\n",
    "        + [f\"feature_{idx:02d}\" for idx in range(79) if idx not in (9, 10, 11, 61)] \\\n",
    "        + [f\"responder_{idx}_lag_1\" for idx in range(9)] \\\n",
    "        + ['sin_time_id', 'cos_time_id','sin_time_id_half_day','cos_time_id_half_day'] \\\n",
    "        + [f'feature_09_cat_{idx}' for idx in range(11)] + [f'feature_10_cat_{idx}' for idx in range(9)] + ['feature_11_cat_0', 'feature_11_cat_1']\n",
    "\n",
    "    categorical_cols = [\"feature_09\", \"feature_10\", \"feature_11\"]\n",
    "    corr_cols = [f\"feature_{i:02d}\" for i in range(21, 32)]\n",
    "    cat_features = ['feature_09_cat_0', 'feature_09_cat_1', 'feature_09_cat_2', 'feature_09_cat_3', 'feature_09_cat_4', \n",
    "        'feature_09_cat_5', 'feature_09_cat_6', 'feature_09_cat_7', 'feature_09_cat_8', 'feature_09_cat_9', \n",
    "        'feature_09_cat_10', 'feature_10_cat_0', 'feature_10_cat_1', 'feature_10_cat_2', 'feature_10_cat_3', \n",
    "        'feature_10_cat_4', 'feature_10_cat_5', 'feature_10_cat_6', 'feature_10_cat_7', 'feature_10_cat_8', \n",
    "        'feature_11_cat_0', 'feature_11_cat_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35152365, 107), (4413112, 107))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/root/autodl-tmp/jane-street-2024/train-validate-set/train_fold_3.parquet'\n",
    "fold_num = path.split('fold_')[1].split('.')[0]\n",
    "print(int(fold_num)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pl.scan_parquet(path).collect().to_pandas()\n",
    "valid = pl.scan_parquet(f\"/root/autodl-tmp/jane-street-2024/train-validate-set/valid_fold_{fold_num}.parquet\").collect().to_pandas()\n",
    "\n",
    "# valid1 = pl.scan_parquet(\"/root/autodl-tmp/jane-street-2024/train-validate-set/valid_fold_1.parquet\").collect().to_pandas()\n",
    "valid2 = pl.scan_parquet(f\"/root/autodl-tmp/jane-street-2024/train-validate-set/valid_fold_{str(int(fold_num) - 1)}.parquet\").collect().to_pandas()\n",
    "train.shape, valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[CONFIG.cat_features] = train[CONFIG.cat_features].astype(int)\n",
    "valid[CONFIG.cat_features] = valid[CONFIG.cat_features].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[ CONFIG.feature_cols ]\n",
    "X_train = X_train.ffill().fillna(0)\n",
    "y_train = train[ CONFIG.target_col ]\n",
    "w_train = train[\"weight\"]\n",
    "\n",
    "X_valid = valid[ CONFIG.feature_cols ]\n",
    "X_valid = X_valid.ffill().fillna(0)\n",
    "y_valid = valid[ CONFIG.target_col ]\n",
    "w_valid = valid[\"weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pool = Pool(data=X_train, label=y_train, weight=train['weight'])  \n",
    "valid_pool = Pool(data=X_valid, label=y_valid, weight=valid['weight'])  \n",
    "\n",
    "del X_train, y_train, w_train, X_valid, y_valid, w_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(seed):\n",
    "    # CatBoost parameters (defined in the original notebook)\n",
    "    CAT_Params = {\n",
    "        'task_type':'GPU',\n",
    "        'random_state':seed,\n",
    "        'eval_metric':'RMSE',\n",
    "        'bagging_temperature':0.50,\n",
    "        'iterations':200,\n",
    "        'learning_rate':0.1,\n",
    "        'max_depth':12,\n",
    "        'l2_leaf_reg':1.25,\n",
    "        'min_data_in_leaf':24,\n",
    "        'random_strength':0.25,\n",
    "        'early_stopping_rounds': 50,\n",
    "        'verbose':0\n",
    "    }\n",
    "    \n",
    "    # Train the model using cbt.train\n",
    "    model = CatBoost(params)\n",
    "    model.train(train_pool, eval_set=valid_pool)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x7f2841c07880>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_model_on_valid_sets(model, valid_sets, feature_cols, target_col, weight_col):\n",
    "    r2_scores = []\n",
    "    \n",
    "    for valid_df in valid_sets:\n",
    "        # 获取特征并进行相同的预处理\n",
    "        X_valid = valid_df[feature_cols].ffill().fillna(0)  # 添加与之前相同的预处理步骤\n",
    "        y_valid = valid_df[target_col]\n",
    "        w_valid = valid_df[weight_col]\n",
    "        \n",
    "        # Convert to DMatrix\n",
    "        dvalid = lgb.Dataset(X_valid, label=y_valid, weight=w_valid)\n",
    "        \n",
    "        # 进行预测\n",
    "        y_pred_valid = model.predict(dvalid)\n",
    "        \n",
    "        # 计算 R² 值\n",
    "        valid_score = r2_score(y_valid, y_pred_valid, sample_weight=w_valid)\n",
    "        r2_scores.append(valid_score)\n",
    "    \n",
    "    return r2_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for hyperparameter optimization\n",
    "    \"\"\"\n",
    "    # Define hyperparameter search space\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 10.0, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),  # Added for LightGBM\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 20),  # Added for LightGBM\n",
    "        \n",
    "        # Fixed parameters\n",
    "        'random_state': CONFIG.seeds[int(fold_num)],\n",
    "        'metric': 'rmse',  # Evaluation metric\n",
    "        'boosting_type': 'gbdt',  # Gradient Boosting Decision Tree\n",
    "        'device': 'gpu',  # Use GPU if available\n",
    "    }\n",
    "\n",
    "    # Train the model using LightGBM\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=400,  # Number of boosting rounds\n",
    "        valid_sets=[dvalid],  # Evaluation set\n",
    "        early_stopping_rounds=10,  # Early stopping\n",
    "        verbose_eval=False  # Verbose output\n",
    "    )\n",
    "    \n",
    "    # Evaluate on all validation sets\n",
    "    valid_sets = [valid]\n",
    "    r2_scores = evaluate_model_on_valid_sets(\n",
    "        model, valid_sets, CONFIG.feature_cols, CONFIG.target_col, 'weight'\n",
    "    )\n",
    "    mean_r2 = sum(r2_scores) / len(r2_scores)\n",
    "    \n",
    "    \n",
    "    # 打印当前试验的结果\n",
    "    print(f\"\\nTrial {trial.number}:\")\n",
    "    print(f\"R2 scores: {r2_scores}\")\n",
    "    print(f\"Mean R2: {mean_r2}\")\n",
    "    \n",
    "    return mean_r2\n",
    "\n",
    "def run_optuna_optimization(n_trials=100):\n",
    "    \"\"\"\n",
    "    运行Optuna优化\n",
    "    \"\"\"\n",
    "    # 创建study对象\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=TPESampler(seed=CONFIG.seeds[int(fold_num)]),\n",
    "        study_name=\"xgboost_optimization\"\n",
    "    )\n",
    "    \n",
    "    # 运行优化\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    \n",
    "    # 打印结果\n",
    "    print(\"\\nBest trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(\"  Value: \", trial.value)\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    \n",
    "    # 可视化结果\n",
    "    try:\n",
    "        # 参数重要性\n",
    "        optuna.visualization.plot_param_importances(study)\n",
    "        plt.show()\n",
    "        \n",
    "        # 优化历史\n",
    "        optuna.visualization.plot_optimization_history(study)\n",
    "        plt.show()\n",
    "        \n",
    "        # 参数关系\n",
    "        optuna.visualization.plot_parallel_coordinate(study)\n",
    "        plt.show()\n",
    "    except:\n",
    "        print(\"Visualization failed. Make sure you have plotly installed for better visualizations.\")\n",
    "    \n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 运行优化\n",
    "# study = run_optuna_optimization(n_trials=100)  # 可以根据需要调整trials数量\n",
    "\n",
    "# # 使用最佳参数训练最终模型\n",
    "# best_params = study.best_params\n",
    "# best_params.update({\n",
    "#     'random_state': CONFIG.seeds[int(fold_num)],\n",
    "#     'tree_method': 'gpu_hist',\n",
    "#     'device': 'cuda',\n",
    "#     'n_gpus': 1\n",
    "# })\n",
    "\n",
    "# # 训练最终模型\n",
    "# final_model = LGBMRegressor(**best_params)\n",
    "# X_train = train[CONFIG.feature_cols].ffill().fillna(0)\n",
    "# y_train = train[CONFIG.target_col]\n",
    "# w_train = train['weight']\n",
    "# final_model.fit(X_train, y_train, sample_weight=w_train)\n",
    "\n",
    "# # 评估最终模型\n",
    "# valid_sets = [valid]\n",
    "# final_scores = evaluate_model_on_valid_sets(\n",
    "#     final_model, valid_sets, CONFIG.feature_cols, CONFIG.target_col, 'weight'\n",
    "# )\n",
    "# print(\"\\nFinal Model Scores:\")\n",
    "# print(f\"R2 scores: {final_scores}\")\n",
    "# print(f\"Mean R2: {sum(final_scores) / len(final_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(CONFIG.seeds[int(fold_num)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sets = [valid, valid2]  # 填入验证集（valid, valid1, valid2, valid3）\n",
    "r2_scores = evaluate_model_on_valid_sets(model, valid_sets, CONFIG.feature_cols, CONFIG.target_col, \"weight\")\n",
    "print(\"R² scores for the validation sets:\", r2_scores)\n",
    "print(\"R^2 scores mean:\", sum(r2_scores) / len(r2_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先准备评估集的数据\n",
    "X_valid1 = valid2[CONFIG.feature_cols].ffill().fillna(0)\n",
    "y_valid1 = valid2[CONFIG.target_col]\n",
    "\n",
    "def continue_training(model, d_valid, eval_sets=None, early_stopping_rounds=10):\n",
    "    \"\"\"\n",
    "    更安全的继续训练方法\n",
    "    \n",
    "    Args:\n",
    "        model: LightGBM模型\n",
    "        d_valid: 当前验证集\n",
    "        eval_sets: 包含评估集的列表，格式为 [(X_valid1, y_valid1)]\n",
    "        early_stopping_rounds: 早停轮数\n",
    "    \"\"\"\n",
    "    # Define original parameters\n",
    "    params = model.params\n",
    "    original_lr = float(params.get('learning_rate', 0.1))\n",
    "    \n",
    "    # Update parameters\n",
    "    params.update({\n",
    "        'learning_rate': original_lr * 0.1,\n",
    "        'reg_alpha': float(params.get('reg_alpha', 0)) + 0.5,\n",
    "        'reg_lambda': float(params.get('reg_lambda', 0)) + 0.5,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8\n",
    "    })\n",
    "    \n",
    "    # Prepare evaluation sets\n",
    "    evals = [(d_valid, 'train')]\n",
    "    if eval_sets:\n",
    "        evals += [(lgb.Dataset(X, label=y), f'valid_{i}') for i, (X, y) in enumerate(eval_sets)]\n",
    "    \n",
    "    # Continue training\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        dtrain=model,  # Use the existing model\n",
    "        num_boost_round=500,  # Set a high number for boosting rounds\n",
    "        valid_sets=evals,\n",
    "        early_stopping_rounds=early_stopping_rounds,\n",
    "        verbose_eval=True  # Optional: to see the progress\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Use the updated function\n",
    "eval_sets = [\n",
    "    (X_valid1, y_valid1)\n",
    "]\n",
    "\n",
    "model = continue_training(model, dvalid, eval_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_r2_scores = evaluate_model_on_valid_sets(model, [valid2], CONFIG.feature_cols, CONFIG.target_col, 'weight')\n",
    "print(\"R² scores for the other validation sets:\", new_r2_scores)\n",
    "\n",
    "# new_r2_scores = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "    \"model\" : model,\n",
    "    \"cv\" : [r2_scores, new_r2_scores]\n",
    "}\n",
    "with open(f\"cat_fold_{fold_num}.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(result, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jane_street",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
